# data-eng-study
This repository covers my plan for transitioning from digital analytics to data engineering with a focus on building ML pipelines. Listed below are some steps and advice to help me make this transition smoothly:

### 1. *Strengthen Technical Foundations*
- *Advanced SQL:* I need to deepen my knowledge in SQL, particularly in optimizing queries, working with complex joins, and understanding execution plans.
- *Python for Data Engineering:* I should expand my Python skills to include libraries such as Pandas, NumPy, and Dask for data manipulation, as well as frameworks like Airflow for workflow management.
- *Cloud Platforms:* I am already familiar with GCP and BigQuery. I should explore other services like Dataflow, Pub/Sub, and AI Platform for building ML pipelines.

### 2. *Learn About Data Engineering Tools and Technologies*
- *Data Warehousing Solutions:* I should learn about other data warehousing solutions like Snowflake, Redshift, and Databricks.
- *Big Data Technologies:* I need to familiarize myself with Hadoop, Spark, and Kafka for handling large-scale data processing.
- *Data Transformation Tools:* I should learn about dbt (data build tool) for transforming data within data warehouses.
- *Workflow Orchestration:* I should deepen my understanding of workflow orchestration tools like Apache Airflow for managing and scheduling data pipelines.

### 3. *Understand Machine Learning Pipelines*
- *ML Frameworks:* I should gain experience with ML frameworks like TensorFlow, PyTorch, and scikit-learn.
- *MLOps:* I need to learn about MLOps practices, which involve the integration of ML into the DevOps cycle to automate and streamline ML workflows.
- *Model Deployment:* I should understand different ways to deploy ML models, such as using Docker, Kubernetes, or cloud-based services like GCP AI Platform, AWS SageMaker, or Azure ML.

### 4. *Hands-On Projects*
- *Personal Projects:* I should create end-to-end projects that involve data ingestion, processing, training ML models, and deployment.
- *Contribute to Open Source:* I need to contribute to open-source data engineering and ML projects to build my portfolio and gain real-world experience.

### 5. *Certifications*
- *Cloud Certifications:* I should obtain certifications related to cloud platforms, such as [Google Cloud Professional Data Engineer](./GCP%20Data%20Engineer%20Certification/), AWS Certified Data Analytics â€“ Specialty, or Azure Data Engineer Associate.
- *Data Engineering Certifications:* I should consider certifications specific to data engineering, such as the Databricks Certified Data Engineer or Cloudera Certified Professional Data Engineer.

### 6. *Networking and Community Involvement*
- *Join Communities:* I need to engage with data engineering and ML communities on platforms like GitHub, Stack Overflow, Reddit, and LinkedIn.
- *Attend Conferences and Meetups:* I should participate in industry conferences, webinars, and local meetups to stay updated on the latest trends and network with professionals in the field.

### 7. *Learning Resources*
- *Books:* I should read books such as "[Designing Data-Intensive Applications](https://www.amazon.co.uk/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321)" by Martin Kleppmann, "Data Engineering on Azure" by Vlad Riscutia, and "Data Pipelines with Apache Airflow" by Bas P. Harenslak and Julian Rutger de Ruiter.
- *Online Courses:* I need to enrol in courses on Coursera, Udacity, or edX that focus on data engineering and ML pipelines.
- *Blogs and Tutorials:* I should follow blogs and tutorials from data engineering experts and organizations to learn about best practices and new technologies.