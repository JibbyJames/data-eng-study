# 1.2 Designing for Reliability and Fidelity 

Ensuring reliability and fidelity in data engineering on Google Cloud Platform involves meticulous planning and the use of various tools and services that GCP offers. Let's expand upon the key areas:

## Preparing and Cleaning Data
### Dataprep by Trifacta:
- Overview video: https://www.youtube.com/watch?v=lJpStL2qMyY
- **Functionality:** Dataprep is a data service that allows users to visually explore, clean, and prepare structured and unstructured data for analysis.
- **Capabilities:** It offers intelligent data transformation suggestions, data profiling, and integration with BigQuery for seamless data operations.
- **Use Cases:** Ideal for ad-hoc data preparation tasks, simplifying data wrangling processes, and ensuring that data is ready for downstream analytics.

### Dataflow:
- Overview: [Cloud Dataflow](./../../Services/Cloud%20Dataflow.md)
- **Functionality:** Dataflow is a fully managed stream and batch processing service that supports real-time data integration.
- **Capabilities:** Provides autoscaling, windowing, and exactly-once processing semantics. It integrates well with Apache Beam for a unified programming model.
- **Use Cases:** Suitable for ETL (extract, transform, load) processes, data enrichment, and real-time analytics pipelines.

### Cloud Data Fusion:
- Overview: [Cloud Data Fusion](./../../Services/Cloud%20Data%20Fusion.md)
- **Functionality:** Cloud Data Fusion is a managed data integration service that allows users to build and manage ETL pipelines.
- **Capabilities:** Features a visual interface for pipeline design, a rich library of pre-built connectors, and transformation components. It also supports hybrid and multi-cloud data integration.
- **Use Cases:** Useful for integrating data from various sources, transforming data for analytics, and maintaining data consistency across systems.

## Monitoring and Orchestration of Data Pipelines

### Cloud Composer:
- Overview: [Cloud Data Fusion](./../../Services/Cloud%20Composer.md)
- **Functionality:** Cloud Composer is a managed workflow orchestration service built on Apache Airflow.
- **Capabilities:** Automates the scheduling and monitoring of complex data pipelines, provides alerting and retries for failed - tasks, and integrates with various GCP services like BigQuery, Cloud Storage, and Dataflow.
- **Use Cases:** Orchestrating ETL jobs, managing dependencies between tasks, and ensuring data workflows are executed reliably.

### Stackdriver Monitoring and Logging:
- **Functionality:** Stackdriver provides comprehensive monitoring, logging, and diagnostics for applications on GCP.
- **Capabilities:** Real-time monitoring, custom metrics, log management, alerting, and dashboarding.
- **Use Cases:** Monitoring pipeline performance, diagnosing issues in real-time, and ensuring system health.

## Disaster Recovery and Fault Tolerance

### Cloud Storage:
- **Functionality:** Cloud Storage offers durable and highly available object storage.
- **Capabilities:** Features multi-regional storage options, automatic redundancy, and lifecycle management policies.
- **Use Cases:** Storing backup data, archiving data, and ensuring data durability and availability.

### BigQuery:
- **Functionality:** BigQuery is a fully managed, serverless data warehouse.
- **Capabilities:** Provides automatic replication, high availability, and built-in backup and restore functionalities.
- **Use Cases:** Disaster recovery for analytical data, ensuring high availability of data for BI tools, and minimizing downtime.

### Cloud SQL and Cloud Spanner:
- **Functionality:** Managed relational database services with high availability and disaster recovery features.
- **Capabilities:** Automatic backups, point-in-time recovery, cross-region replication (Cloud Spanner), and failover configurations.
- **Use Cases:** Ensuring the resilience of transactional data stores, maintaining data integrity, and supporting ACID-compliant operations.

## Making Decisions Related to ACID Compliance and Availability

### BigQuery:
- Overview: [BigQuery](./../../Services/BigQuery.md)
- **Capabilities:** While not fully ACID-compliant, it ensures data consistency through eventual consistency and offers strong durability guarantees.
- **Use Cases:** Suitable for analytical workloads where eventual consistency is acceptable.

### Cloud Spanner:
- Overview: [Cloud Spanner](./../../Services/Cloud%20Spanner.md)
- **Capabilities:** Fully ACID-compliant, globally distributed, and provides high availability.
- **Use Cases:** Ideal for mission-critical applications requiring strong consistency, such as financial services and inventory management systems.

### Cloud SQL:
- Overview: [Cloud SQL](./../../Services/Cloud%20SQL.md)
- **Capabilities:** ACID-compliant relational database service with support for MySQL, PostgreSQL, and SQL Server.
- **Use Cases:** Suitable for traditional OLTP workloads, maintaining strong transactional integrity, and supporting web and mobile applications.

## Data Validation

### Dataflow:
- Overview: [Cloud Dataflow](./../../Services/Cloud%20Dataflow.md)
- **Capabilities:** Supports data validation through schema enforcement, data quality checks, and the use of Beam's validation libraries.
- **Use Cases:** Ensuring data correctness during ingestion and transformation processes, validating streaming data in real-time.

### BigQuery:
- Overview: [BigQuery](./../../Services/BigQuery.md)
- **Capabilities:** Allows defining and enforcing schema constraints, conducting anomaly detection, and using SQL for data validation queries.
- **Use Cases:** Post-load data validation, running data quality checks, and identifying data anomalies in large datasets.

### Cloud Data Quality (DQ):
- Overview: [Cloud DQ](./../../Services/Cloud%20Data%20Quality.md)
- **Functionality:** Cloud DQ is a data quality management tool that integrates with BigQuery and other GCP services.
- **Capabilities:** Provides data profiling, rule-based validation, and data quality monitoring.
- **Use Cases:** Continuous data quality assessment, enforcing data governance policies, and ensuring data accuracy and reliability.



## Example Scenarios and Architectures

### Designing for Reliability and Fidelity

#### Scenario 1: Ensuring Data Quality Through Preparation and Cleaning
- **Business Requirement:** The company needs to ensure high data quality for their analytics platform.
- **Solution:** Use Dataprep for data cleaning and transformation tasks, leveraging its visual interface to detect and correct anomalies. Integrate Dataflow for scalable data processing and Cloud Data Fusion for building complex ETL pipelines that ensure data is consistently prepared and cleaned before being used in analytics.
- **Architecture:**
  - **Data Source:** Raw data from various sources (e.g., databases, APIs, flat files).
  - **Data Preparation:** Use Dataprep for initial data cleaning and transformation.
  - **Data Processing:** Use Dataflow for large-scale data processing tasks.
  - **ETL Orchestration:** Use Cloud Data Fusion to orchestrate the ETL workflows.
  - **Data Warehouse:** Store clean data in BigQuery for analytics.

#### Scenario 2: Monitoring and Orchestration of Data Pipelines
- **Business Requirement:** The company requires robust monitoring and orchestration for their data pipelines to ensure reliability.
- **Solution:** Implement Cloud Composer (based on Apache Airflow) for orchestration of data pipelines. Use Stackdriver (now part of Cloud Operations Suite) for monitoring pipeline health, performance, and detecting anomalies.
- **Architecture:**
  - **Pipeline Orchestration:** Use Cloud Composer to manage and schedule data pipeline tasks.
  - **Monitoring:** Integrate Stackdriver to monitor pipeline performance and alert on failures or anomalies.
  - **Data Processing:** Use Dataflow for processing data within the orchestrated pipelines.
  - **Data Storage:** Store processed data in BigQuery and Cloud Storage.

#### Scenario 3: Implementing Disaster Recovery and Fault Tolerance
- **Business Requirement:** The company needs to ensure business continuity with a robust disaster recovery plan for their data infrastructure.
- **Solution:** Design a multi-region disaster recovery strategy using Cloud Storage for data backups, and implement automated failover mechanisms for critical data processing tasks using Dataflow and BigQuery.
- **Architecture:**
  - **Data Backup:** Use Cloud Storage to store regular backups of critical data in multiple regions.
  - **Automated Failover:** Configure Dataflow jobs with failover capabilities to switch to secondary processing nodes in case of failures.
  - **Data Replication:** Replicate BigQuery datasets across regions to ensure availability and durability.
  - **Recovery Testing:** Regularly test the disaster recovery plan to ensure quick recovery in case of an actual disaster.

#### Scenario 4: Ensuring ACID Compliance and High Availability
- **Business Requirement:** The company requires strict ACID compliance for their transactional data and high availability for their data services.
- **Solution:** Use Cloud Spanner for scalable, globally-distributed databases that ensure ACID properties and high availability. Complement with Bigtable for high-throughput and low-latency workloads.
- **Architecture:**
  - **Transactional Data:** Store in Cloud Spanner to ensure ACID compliance.
  - **High-Throughput Data:** Use Bigtable for high-speed read and write operations.
  - **Data Integration:** Integrate both Cloud Spanner and Bigtable with Dataflow for processing and BigQuery for analytics.
  - **Service Availability:** Deploy across multiple regions to ensure high availability and fault tolerance.

#### Scenario 5: Implementing Data Validation
- **Business Requirement:** The company needs to ensure that all ingested data meets specific quality standards before processing.
- **Solution:** Use Dataflow and Cloud Data Fusion to implement data validation checks at various stages of the ETL pipeline. Employ Dataform for validating data within BigQuery.
- **Architecture:**
  - **Data Ingestion:** Use Dataflow to ingest data from various sources.
  - **Data Validation:** Implement validation checks in Dataflow to ensure data quality.
  - **ETL Orchestration:** Use Cloud Data Fusion to manage the end-to-end ETL workflow with integrated validation steps.
  - **Data Storage:** Store validated data in BigQuery.
  - **Validation in BigQuery:** Use Dataform to run SQL-based validation tests on datasets within BigQuery to ensure they meet required standards.

By focusing on these scenarios and architectures, you will understand how to design reliable and high-fidelity data systems on Google Cloud Platform, ensuring data integrity, availability, and compliance with business requirements.




## ACID - Atomicity, Consistency, Isolation, and Durability

ACID is an acronym that stands for Atomicity, Consistency, Isolation, and Durability. These are a set of properties that ensure reliable processing of database transactions. Each of these properties plays a crucial role in maintaining the integrity and reliability of a database.

**Atomicity:** This property ensures that a transaction is all or nothing. A transaction is an indivisible unit of work. If any part of the transaction fails, the entire transaction is rolled back, and the database remains unchanged. This prevents partial updates that could lead to data corruption.

**Consistency:** Consistency ensures that a transaction brings the database from one valid state to another valid state, maintaining all predefined rules and constraints. After the transaction completes, all data must be in a valid state as defined by the database schema, rules, and constraints.

**Isolation:** Isolation ensures that the operations of a transaction are invisible to other transactions until the transaction is complete. This prevents transactions from interfering with each other and ensures that concurrent transactions execute as if they were run sequentially, maintaining the integrity of the database.

**Durability:** Durability guarantees that once a transaction has been committed, it will remain so, even in the event of a system failure. This means that the changes made by a transaction are permanently recorded in the database and will survive any subsequent failures.